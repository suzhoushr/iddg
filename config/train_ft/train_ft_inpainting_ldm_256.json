{
    "name": "ft_dehongcheng_ldm_sz256_label1000", // experiments name
    "gpu_ids": [0], // gpu ids list, default is single 0
    "seed" : -1, // random seed, seed <0 represents randomization not used 
    "finetune_norm": false, // find the parameters to optimize

    "path": { //set every part file path
        "base_dir": "experiments", // base path for all log except resume_state
        "code": "code", // code backup
        "tb_logger": "tb_logger", // path of tensorboard logger
        "results": "results",
        "checkpoint": "checkpoint",
        // "encoder_resume_state": "experiments/autoencoder/model_epoch_119.pth", 
        "resume_state": "experiments/train_ldm_sz256_label1000_ch192/checkpoint/848"
        // "resume_state": null // ex: 100, loading .state  and .pth from given epoch and iteration        
    },

    "datasets": { 
        "train": { 
            "which_dataset": {  
                "name": ["data.dataset", "InpaintDataset"], 
                "args":{ 
                    "data_root": "/home/data0/project-datasets/dehongcheng/exp_SYB/train_val.flist",
                    "image_size": [256, 256],
                    "class_name": ["cashang", "pengshang", "zangwu", "huashang", "duoliao", "qikong", "zazhi", "jiagongbuliang",
                                   "wudaojiao", "liewen", "qipi", "lvxie", "lengge", "yashang", "queliao"]        
                } 
            },
            "dataloader":{
                "validation_split": 40, 
                "args":{ 
                    "batch_size": 8, 
                    "num_workers": 8,
                    "shuffle": true,
                    "pin_memory": true,
                    "drop_last": true
                },
                "val_args":{ 
                    "batch_size": 8, 
                    "num_workers": 8,
                    "shuffle": false,
                    "pin_memory": true,
                    "drop_last": false
                }
            }
        },
        "test": { 
            "which_dataset": {
                "name": "InpaintDataset", 
                "args":{
                    "data_root": "datasets/hw/test_hwpc_all256.flist",
                    "image_size": [256, 256]
                }
            },
            "dataloader":{
                "args":{
                    "batch_size": 8,
                    "num_workers": 8,
                    "pin_memory": true
                }
            }
        }
    },
    "model": { 
        "which_model": {
            "name": ["models.model", "MaLiang"], 
            "args": {
                "sample_num": 8, 
                "task": "inpainting",
                "stage": "finetune", // train | finetune
                "lora_config": {
                    "flag": "not_use",  // ft | infer | not_use
                    "loras": null,
                    "target_replace_module": ["CrossAttention", "Attention", "GEGLU"],
                    "inject_emb": true,
                    "lora_rank": 64,
                    "lora_dropout_p": 0.1,
                    "lora_scale": 2.0,
                    "lora_steps": 200,
                    "lora_lr": 5e-5
                },
                // "ema_scheduler": {
                //     "ema_start": 1,
                //     "ema_iter": 1,
                //     "ema_decay": 0.9999
                // },
                "optimizers": [
                    { "lr": 5e-5, "weight_decay": 0} 
                ]
            }
        }, 
        "which_networks": [ 
            {
                "name": ["models.ddpm", "DDPM"],
                "args": { 
                    "init_type": "kaiming", 
                    "module_name": "sd_v15", 
                    "sample_type": "ddim",
                    "sample_timesteps": 300,
                    "use_cond": false,
                    "use_ldm": true,
                    "ddconfig": {
                        "double_z": true,
                        "z_channels": 4,
                        "resolution": 256,
                        "in_channels": 3,
                        "out_ch": 3,
                        "ch": 128,
                        "ch_mult": [1, 2, 4, 4],
                        "num_res_blocks": 2,
                        "attn_resolutions": [],
                        "dropout": 0.0
                    },                    
                    "cond": {
                        "context_dim": 768,
                        "num_classes": 1000
                    },
                    "unet": {
                        "image_size": 32, 
                        "in_channels": 5, 
                        "model_channels": 192,
                        "out_channels": 4, 
                        "num_res_blocks": 2,  
                        "attention_resolutions": [ 
                            4,
                            2,
                            1
                        ],
                        "dropout": 0.1,                 
                        "channel_mult": [                  
                            1,
                            2,
                            4,
                            4
                        ],
                        "conv_resample": true,              // true
                        "dims": 2,                           // None
                        "num_classes": 1000,                // 100
                        "use_checkpoint": false,            // false
                        "use_fp16": false,                  // false
                        "num_heads": 8,                     // 1
                        "num_head_channels": -1,            // 32
                        "num_heads_upsample": -1,           // -1
                        "use_scale_shift_norm": true,      // true
                        "resblock_updown": true,           // true, it is very strange that the loss could not decrease when this value set to false 
                        "use_new_attention_order": false,   // false
                        "use_spatial_transformer": false,    // custom transformer support
                        "transformer_depth": 1,             // custom transformer support
                        "context_dim": null,                 // custom transformer support
                        "n_embed": null,                    // custom support for prediction of discrete ids into codebook of first stage vq model
                        "legacy": false                     // None
                    },
                    "beta_schedule": {
                        "train": {
                            "schedule": "linear",
                            "n_timestep": 1000,   // original is 2000
                            "linear_start": 1e-4,
                            "linear_end": 0.02
                        },
                        "test": {
                            "schedule": "linear",
                            "n_timestep": 1000,
                            "linear_start": 1e-4,
                            "linear_end": 0.02
                        }
                    }
                }
            }
        ],
        "which_losses": [ 
            "mse_loss"
        ],
        "which_metrics": [ 
            "mae" 
        ]
    },

    "train": { 
        "n_epoch": 100, 
        "n_iter": 1e8, 
        "val_epoch": 1, 
        "save_checkpoint_epoch": 10,
        "log_iter": 100, 
        "tensorboard" : true 
    },
    
    "debug": { 
        "val_epoch": 1,
        "save_checkpoint_epoch": 1,
        "log_iter": 2,
        "debug_split": 50 
    }
}
